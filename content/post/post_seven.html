---
title: "Monte Carlo Simulations and Risk Management"
author: "Nicolas Cuesta Quintero"
date: 2021-08-29T21:00:00-05:00
categories: ["R"]
tags: ["risk management", "Monte Carlo", "cholesky decomposition", "pca", "principal components analysis", "rstats"]
---



<p>Managing portfolio price risk is one the main headaches of asset managers. When there is a single source of risk, measuring market risk becomes simple, and a straightforward approach can be used to calculate the value at risk (VaR) of a portfolio. However, this is usually not the case. Asset managers face multiple sources of risk, what increases the complexity of financial models rapidly. Simulating correlated sources of risk could lead to misspecifications of the model, the computational requirements of Monte Carlo methods increase with the number of assets, and the mathematical toolkit needs to be upgraded beyond a linear combination of known terms. In this post, I will show you how Ito’s process can be used to simulate the price path of individual assets. Moreover, I will show you how to simulate the price path of multiple assets. Even though this would be an easy task if the portfolio assets had independent returns by assumption, we are going to use the Cholesky decomposition of the correlation matrix to manage the price risk of portfolios with correlated assets.</p>
<p>We will use the following libraries:</p>
<pre class="r"><code>library(tidyverse)</code></pre>
<div id="a-unique-source-of-risk" class="section level2">
<h2>A unique source of risk</h2>
<p>Modeling the risk factors of an investment requires to understand what is the stochastic behavior of the assets that form the portfolio. In other words, what is the stochastic process that is followed by the asset price. If a risk analyst knows how the price of the asset may evolve in the future, he can simulate multiple scenarios, calculate different percentiles, and get an idea of what could be the worst expected loss. This would require, however, to know the probability distribution of the price, or its returns. A common assumption based on empirical observations of stocks and currencies returns, is that returns are independently and identically distributed, following a normal distribution. Therefore, a Markov process can be used for stocks and currencies returns, or bonds yields, since their individual returns path are independent of their own history. Nevertheless, there are more stylized facts to be explained. In fact, stocks usually have a long-term trend, while bond prices revert to the face value by their maturity date. This led to a special type of processes that are used for simulating assets price paths, the Ito’s process.</p>
<div id="itos-process" class="section level3">
<h3>Ito’s process</h3>
<p>Given an asset <span class="math inline">\(x\)</span>, an Ito process describes the change of its price as</p>
<p><span class="math display">\[\begin{equation}
\Delta x_{t+1}=x_{t+1}-x_{t}=a(x,t)\Delta t+b(x,t)\Delta z_{t}.
\end{equation}\]</span></p>
<p>Here, <span class="math inline">\(t\)</span> stands for the time and <span class="math inline">\(\Delta z_{t}\sim N(0,\Delta t)\)</span> is a Weiner process. The function <span class="math inline">\(a\)</span> represent the mean behavior, while function <span class="math inline">\(b\)</span> the volatility of the process. Notice that the definition of <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> is what characterize the stylized facts of assets returns.</p>
<p>For stocks and exchange rates, whose price is denoted by <span class="math inline">\(S\)</span>, we use the Geometric Brownian Montion (GBM). This is the particular case of a random walk with drift, i.e., we add a trend to the random walk. We define <span class="math inline">\(a=\mu S\)</span>, where <span class="math inline">\(\mu\)</span> is the long-term average return continuously compounded, and <span class="math inline">\(b=\sigma S\)</span>, where <span class="math inline">\(\sigma\)</span> the long-term (unconditional) volatility. Notice that the volatility term <span class="math inline">\(b\)</span> scales with the asset price so its unlikely to have negative stock prices, what would be expected due to the limited liability of shareholders. The GBM is given by</p>
<p><span class="math display">\[\Delta S_{t+1}=\mu S_{t}\Delta t+\sigma S_{t}\Delta z_{t}.\]</span></p>
<p>In practice, the limit of the model is taken (<span class="math inline">\(dS\rightarrow 0\)</span>), so that <span class="math inline">\(ln|S|\)</span> follows a normal distribution and <span class="math inline">\(S\)</span> a log-normal distribution. This implies that over an interval of time <span class="math inline">\(\Delta t\)</span>, the ending price is</p>
<p><span class="math display">\[\begin{equation}
S_{t+1}=S_{t}e^{(\mu-\sigma^{2}/2)\Delta t +\sigma z_{t}}.
\end{equation}\]</span></p>
<p>For interest rates we would expect a mean-reversion process instead. The GBM possess a drift, or a trend, and does not fit the empirical data observed in bonds yields. A second type of Ito process, called the one factor Ornstein-Uhlenbeck process, is used for modelling interest rates and commodities. This is the case of a bond that matches its face value at its maturity date, assuming there is no default in the loan payments. In general, it can be written as</p>
<p><span class="math display">\[\begin{equation}
\Delta r_{t}=\kappa(\theta-r_{t})\Delta t+\sigma r_{t}^{\gamma}\Delta z_{t},
\end{equation}\]</span></p>
<p>where <span class="math inline">\(0\leq\kappa&lt;1\)</span>, <span class="math inline">\(\theta&gt;0\)</span> and <span class="math inline">\(\sigma\geq 0\)</span>. Notice that this is a mean reverting process. When interest rates are higher than the mean value <span class="math inline">\(\theta\)</span>, the difference is negative, and the value moves towards the mean level. Vice versa when interest rates are lower than the mean value. Two of the most known methods that derives from this one are the Vacisek model, when <span class="math inline">\(\gamma =0\)</span>, and the Hull and White model, that also assumes <span class="math inline">\(\theta=\theta(t)\)</span> to model the current term structure of interest rates.</p>
<p>It is worth to mention that one of the biggest markets has been left out, the real estate market. Markov processes, and therefore Ito’s process, are not recommended to simulate real estate capital returns. Observations of real estate price indices, such as NCREIF NPI or UK IPD, have shown positive serial correlation of returns, so today’s price depends on past values. Methods for simulating real estate prices often create an exponential weighted average of previous index values and an underlying efficient market process (GBM), but that would be a conversation that drives us out of scope.</p>
</div>
<div id="generating-random-numbers" class="section level3">
<h3>Generating random numbers</h3>
<p>Previous processes simulation depends on the generation of random numbers. However, computers cannot generate random numbers, but algorithms have been developed to generate pseudo-random numbers, i.e., numbers that look and feel like random ones, but they are not. In the words of Donald Knuth, <em>“random numbers should not be generated with a method chosen at random. Some theory should be used.”</em>. There are multiple methods to generate random numbers that follow a specific probability distribution, but most of them start with the uniform distribution.</p>
<p>Say, you want to generate numbers <span class="math inline">\(U_{n}\)</span> that follow a uniform distribution between 0 and 1. The strategy is to generate integers in <span class="math inline">\([0,m]\)</span> and the uniform numbers will given by</p>
<p><span class="math display">\[U_{n}=\frac{X_{n}}{m}.\]</span></p>
<p>Then, the question is how to generate random integers in the proposed interval. The most common method, introduced by Lehmer in 1949, is called the Linear Congruence Method. You need to pick four “magic numbers”:</p>
<ul>
<li>The modulus <span class="math inline">\(m\)</span>, such that <span class="math inline">\(0&lt;m\)</span>;</li>
<li>The multiplier <span class="math inline">\(a\)</span>, such that <span class="math inline">\(0\leq a&lt;m\)</span>;</li>
<li>The increment <span class="math inline">\(c\)</span>, such that <span class="math inline">\(0\leq c&lt;m\)</span>;</li>
<li>The seed <span class="math inline">\(X_{0}\)</span>, such that <span class="math inline">\(0\leq X_{0}&lt;m\)</span>.</li>
</ul>
<p>The desired sequence of uniform numbers is</p>
<p><span class="math display">\[X_{n+1}=(aX_{n}+c)\,\text{mod}\, m.\]</span></p>
<p>The main problem with the linear congruence method is the selection of the magic numbers. One issue that arise is that the sequence will sooner or later get into a loop, and repeat itself endlessly, as shown below.</p>
<pre class="r"><code># Linear Congruence random generator
linear_congruence &lt;- function(m, a, cc, x0, n = 1) {
  if (sum(m &lt; c(a, cc, x0)) &gt; 0) stop(&quot;Conditions are not satisfied.&quot;)
  if (sum(0 &gt; c(a, cc, x0)) &gt; 0) stop(&quot;Conditions are not satisfied.&quot;)
  x &lt;- vector(&quot;double&quot;); x[1] &lt;- x0
  for (i in 1:n) x[i+1] &lt;- (a * x[i] + cc) %% m
  return(x / m)
}

# Plot
linear_congruence(2 ^ 32, 1103515245, 12345, 241546, 10000) %&gt;% 
  enframe(name = &quot;n&quot;, value = &quot;U&quot;) %&gt;%
  ggplot(mapping = aes(x = n, y = U)) +
  geom_point(size = 0.5)</code></pre>
<p><img src="/post/post_seven_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<p>In conclusion, with the right choice of magic numbers you can create a long data set following a uniform distribution in the interval <span class="math inline">\([0,1]\)</span>, but this does not solve the initial problem about generating the normal distributed numbers used in the Weiner process, <span class="math inline">\(z_{t}\)</span>. Notice that we can write <span class="math inline">\(z_{t}=\sqrt{\Delta t}\,\epsilon_{t}\)</span>, where <span class="math inline">\(\epsilon_{t}\sim N(0,1)\)</span>, so we can focus on generating numbers for the standardize normal distribution. The proposed method in this post is called the inversion method, because is the most common one used by financial engineers. It exploits the fact that one-to-one cumulative distribution functions for continuous random variables, such as the normal, can be inverted to find the corresponding sequence of random numbers with the new distribution, from a sequence of uniform numbers.</p>
<p>Say <span class="math inline">\(U\sim U(0,1)\)</span>. We want to generate random numbers <span class="math inline">\(X\)</span> with cdf <span class="math inline">\(F_{X}\)</span>. Define <span class="math inline">\(Y=F_{X}^{-1}(U)\)</span>, then</p>
<p><span class="math display">\[F_{Y}(y)=P(Y&lt;y)=P(F_{X}^{-1}(U)&lt;y)=P(U&lt;F_{X}(y)).\]</span></p>
<p>Notice that <span class="math inline">\(P(U&lt;F_{X}(y))\)</span> calculates the <span class="math inline">\(X\)</span> cdf for a number <span class="math inline">\(y\)</span>, but returns that same number when compared to <span class="math inline">\(U\)</span>, because this latter random variable is uniform. In other words, <span class="math inline">\(P(U&lt;F_{X}(y))=F_{X}(y)\)</span>. This implies that <span class="math inline">\(F_{Y}(y)=F_{X}(y)\)</span> and, therefore, <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> have the same probability distribution. Thus, you can evaluate <span class="math inline">\(F_{X}^{-1}\)</span> with the uniform distributed data set created with the linear congruence algorithm, and obtain numbers that distribute like <span class="math inline">\(X\)</span>. If <span class="math inline">\(X\)</span> is an standardize normal random variable and random <span class="math inline">\(u\)</span> uniform number, we can find the corresponding <span class="math inline">\(x\)</span> by finding the root of</p>
<p><span class="math display">\[f(x)=F_{X}(x)-u.\]</span></p>
<p>This root can be find with one of the methods shown in previous posts (bisection or secant methods). Since there is no analytical solution for the normal cdf, a numerical method is also required for solving the integral (e.g. Simpson 3/8).</p>
<pre class="r"><code># Bisection method function
bisection &lt;- function(a, b, f, n = 100, tol = 1e-04, show = FALSE) {
  if (f(a) * f(b) &gt; 0) return(&quot;Method conditions not satisfied.&quot;)
  for (k in 1:n) {
    c &lt;- (a + b) / 2
    if (show) cat(&quot;k: &quot;, k, &quot;c:&quot;, c, &quot;f: &quot;, f(c), &quot;\n&quot;)
    if (f(c) == 0 || ((b - a) / 2) &lt; tol) return(c)
    if (f(a) * f(c) &lt; 0) {
      b &lt;- c
    } else {
      a &lt;- c
    }
  }
  cat(&quot;No solution found after &quot;, n, &quot; iterations.&quot;)
}

# Standard normal pdf
norm_pdf &lt;- function(x) exp(-0.5 * x ^ 2) / sqrt( 2 * pi)

# Standard normal cdf
norm_cdf &lt;- function(x) {
  integrate(norm_pdf, lower = -100, upper = x)[1]$value
}

# Inversion method
inv_method &lt;- function(n, cdf) {
  unif_numbers &lt;- linear_congruence(2^32,1103515245,12345,241546,n)
  sapply(unif_numbers, function(u) {
    bisection(-10, 10, function(x) cdf(x) - u)
  })
}

# Plot
inv_method(10000, norm_cdf) %&gt;% 
  enframe(name = &quot;n&quot;, value = &quot;N&quot;) %&gt;%
  ggplot(mapping = aes(x = n, y = N)) +
  geom_point(size = 0.5)</code></pre>
<p><img src="/post/post_seven_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<p>Notice that most of the dots lie in the middle (as seen from the y-axis). This is consistent with the bell shape of the standardize normal distribution. Also, numbers above the 7,500 iterations must be disregarded, because the linear congruence method fell into a loop. Finally, keep in mind that the code developed in this section is trying to replicate, for educational purposes, what a function like <em>stats::rnorm</em> would do. However, it is because the way it was structured. In the following sections we will use R core function instead.</p>
</div>
<div id="simulating-one-asset-price-path" class="section level3">
<h3>Simulating one asset price path</h3>
<p>Consider a stock that follows a Geometric Brownian Motion, with an initial price of <span class="math inline">\(S_{0}=100\)</span>. Let’s assume that the annual continuously compounded returns and volatility are, respectively, <span class="math inline">\(\mu=8\%\)</span>, <span class="math inline">\(\sigma=14\%\)</span>. We can use the solution to the GBM equation to simulate the price path for the next 20 days. Standardized normal random numbers can be generated with the inversion method and <span class="math inline">\(t=1/252=0.00397\)</span>.</p>
<pre class="r"><code>set.seed(20)

# Function that simulates the path price for a stock
sim_spot &lt;- function(S, mu, sigma, dt = 1 / 252, n = 20L) {
  price_path &lt;- vector(&quot;double&quot;, n + 1L); price_path[1] &lt;- S
  for (i in 2L:(n + 1L)) {
    price_path[i] &lt;- price_path[i-1] * 
      exp(
        (mu - sigma ^ 2 / 2) * dt + sigma * sqrt(dt) * rnorm(1)
      )
  }
  return(price_path)
}

# Plot of four different simulations
map_dfr(
  1:4,
  function(x) {
    tibble(
      Simulation = x,
      Day = 0:20,
      Price = sim_spot(100, 0.08, 0.14)
    )
  }
) %&gt;% 
  mutate(Simulation = factor(str_c(&quot;Trial - &quot;, Simulation))) %&gt;% 
  ggplot(mapping = aes(x = Day, y = Price)) +
  geom_line() +
  geom_point() +
  facet_wrap(~Simulation)</code></pre>
<p><img src="/post/post_seven_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<p>Notice that simulations are based on the normal distribution. However, empirical observations for multiple markets show that asset returns have fat tails (<em>Kurtosis&gt;3</em>), which is not the case for the normal distribution. In other words, these types of simulations may underestimate the probability and the negative effect of a market drop, so the asset manager exposure would probably be bigger than simulated. However, the outlier returns could not necessarily be due to fat tail returns. They could belong to a multivariate normal or could be the effect of a dynamic volatility. Anyway, multiple methods have been developed to deal with this limitation, being <a href="https://en.wikipedia.org/wiki/Copula_(probability_theory)">Copulas</a> one the most common.</p>
</div>
</div>
<div id="multiple-sources-of-risk" class="section level2">
<h2>Multiple sources of risk</h2>
<div id="the-simple-case" class="section level3">
<h3>The simple case</h3>
<p>If the assets returns were independent and identically distributed, the problem of simulating multiple assets is straightforward. The independence implies that the linear correlation is zero, and the method explained above can be repeated for each individual asset. Thus,</p>
<p><span class="math display">\[\Delta S_{t+1,j}=\mu_{j} S_{t,j}\Delta t+\sigma_{t} S_{t,j}\Delta z_{t,j}.\]</span></p>
<p>where <span class="math inline">\(j=1,2,\dots,N\)</span> stands for each of the <span class="math inline">\(N\)</span> assets modeled.</p>
</div>
<div id="two-correlated-assets" class="section level3">
<h3>Two correlated assets</h3>
<p>We know how to sample from two independent univariate normal distributions <span class="math inline">\(\eta_{1},\eta_{2}\sim N(0,1)\)</span>, such that <span class="math inline">\(cov[\eta_{1},\eta_{2}]=0\)</span>. Just repeat the process of previous section twice. What we need is to simulate two correlated standard normal random variables <span class="math inline">\(\epsilon_{1}\)</span> and <span class="math inline">\(\epsilon_{2}\)</span> such that their variances remain <span class="math inline">\(V[\epsilon_{1}]=V[\epsilon_{2}]=1\)</span> and their covariance <span class="math inline">\(cov[\epsilon_{1},\epsilon_{2}]=\rho\)</span>. For this purpose, we create the linear system</p>
<p><span class="math display">\[
\begin{bmatrix}
\epsilon_{1}\\
\epsilon_{2}
\end{bmatrix}
=
\begin{bmatrix}
\alpha_{11} &amp; \alpha_{12} \\
\alpha_{21} &amp; \alpha_{22}
\end{bmatrix}
\begin{bmatrix}
\eta_{1}\\
\eta_{2}
\end{bmatrix}.
\]</span></p>
<p>We can assume without loss of generality that <span class="math inline">\(\alpha_{12}=0\)</span>. Then, from the three conditions on the variances and the covariance, we deduce that <span class="math inline">\(\alpha_{21}=\rho\)</span> and <span class="math inline">\(\alpha_{22}=(1-\rho^{2})^{1/2}\)</span> (three unknowns, three equations). Thus,</p>
<p><span class="math display">\[
\begin{bmatrix}
\epsilon_{1}\\
\epsilon_{2}
\end{bmatrix}
=
\begin{bmatrix}
1 &amp; 0 \\
\rho &amp; (1-\rho^{2})^{1/2}
\end{bmatrix}
\begin{bmatrix}
\eta_{1}\\
\eta_{2}
\end{bmatrix}.
\]</span></p>
<p>We found a way to simulate the two standard normal random variables such that they have a given correlation. We can apply previous equation to each pair of random numbers of the standardize normal sequences, and conclude the correct sequence of the bivariate distribution for the Ito process.</p>
</div>
<div id="cholesky-decompositon" class="section level3">
<h3>Cholesky Decompositon</h3>
<p>Notice one thing of previous example,</p>
<p><span class="math display">\[\begin{bmatrix}
1 &amp; 0 \\
\rho &amp; (1-\rho^{2})^{1/2}
\end{bmatrix}
\begin{bmatrix}
1 &amp; 0 \\
\rho &amp; (1-\rho^{2})^{1/2}
\end{bmatrix}^{T}
=
\begin{bmatrix}
1 &amp; \rho \\
\rho &amp; 1
\end{bmatrix}=\Sigma.\]</span></p>
<p>This means that the lower triangular matrix that allows us to determine correlated sequences of random numbers, multiplied times itself transposed, lead to the covariance matrix between the two random sequences <span class="math inline">\(\epsilon_{1}\)</span> and <span class="math inline">\(\epsilon_{2}\)</span>. This is an important result, because Cholesky factorization states the semi-definite positive matrices, like the covariance matrix, can be written as the product of a lower triangular matrix and itself transposed. In conclusion, we can generalize the example of two random sequences to any number <span class="math inline">\(n\)</span> just by calculating the Cholesky factorization of the covariance matrix between the returns of the assets.</p>
<p>Say, <span class="math inline">\(\Sigma\)</span> is the covariance matrix between the assets we want to simulate. We find its Cholesky decomposition <span class="math inline">\(\Sigma=LL^{T}\)</span> using a numerical algorithm, like explained below. The random numbers we want will belong to a sequences of vectors, each of them defined as <span class="math inline">\(\epsilon=L\eta\)</span>, where <span class="math inline">\(\eta\)</span> is each one of the vectors of the sequence of independent random numbers. Notice that</p>
<p><span class="math display">\[V[\epsilon]=E[\epsilon\epsilon^{T}]=E[(L\eta)(L\eta)^{T}]=LE[\eta\eta^{T}]L^{T}=LV[\eta]L^{T}=\Sigma.\]</span></p>
<p>It is worth to mention that R provides the function <em>base::chol</em>, based on LINPACK package, to calculate the Cholesky decomposition. This function makes the proper verifications before calculating the left diagonal matrix. However, to fully demomnstrate the method from scratch, I follow a variant of Gaussian elimination provided by (Ackleh, Allen, Hearfott, &amp; Seshaiyer, 2009). According to this method, if <span class="math inline">\(A\)</span> is an <span class="math inline">\(n\times n\)</span> semi-definite positive matrix, we set <span class="math inline">\(l_{j1}=a_{j1}\)</span> for <span class="math inline">\(j=1,\dots , n\)</span>. Then, for <span class="math inline">\(i=1,\dots , n\)</span>, set</p>
<p><span class="math display">\[l_{ii}=\left[a_{ii}-\sum_{k=1}^{i-1}l_{ik}^{2}\right]^{1/2};\]</span></p>
<p><span class="math display">\[l_{ji}=\frac{1}{l_{ii}}\left[a_{ji}-\sum_{k=1}^{i-1}l_{ik}l_{jk}\right]^{1/2},\,\text{for}\,\, i+1\leq j\leq n.\]</span></p>
<pre class="r"><code># Function that decompose as semi-definite positive matrix
# using Cholesky factorization
cholesky_decom &lt;- function(A) {
  
  # Check that A is a square semi-definite positive matrix
  
  n &lt;- ncol(A)
  L &lt;- matrix(rep(0, n * n), ncol = n)
  L[1:n, 1] &lt;- A[1:n, 1] / sqrt(A[1, 1])
  for (i in 2:n) {
    j &lt;- i
    while (j &lt;= n) {
      if (i == j) {
        L[i, i] &lt;- sqrt(A[i, i] - sum((L[i, 1:(i-1)]) ^ 2))
      } else {
        L[j, i] &lt;- (A[j, i]-sum(L[i, 1:(i-1)]*L[j, 1:(i-1)]))/L[i, i]
      }
      j &lt;- j + 1
    }
  }
  
  return(L)
}

# Function that simulates the path price for n correlated stocks
sim_spot &lt;- function(S, mu, sigma, dt = 1 / 252, n = 20L) {

  number_assets &lt;- length(S)
  
  # Generate correlated normal random numbers
  L &lt;- cholesky_decom(sigma)
  epsilon &lt;- lapply(1:n, function(x) {
    as.vector(L %*% as.matrix(rnorm(number_assets)))
  })
  
  # Simulate prices path
  price_path &lt;- matrix(nrow = n + 1L, ncol = number_assets)
  price_path[1, 1:number_assets] &lt;- S
  for (i in 2:(n + 1)) {
    price_path[i,1:number_assets] &lt;- price_path[i-1,1:number_assets] *
      exp(
        (mu - (diag(sigma) ^ 2) / 2) * dt + 
          diag(sigma) * sqrt(dt) * epsilon[[i - 1]]
      )
  }
  
  return(price_path)
}

# Plot of four different simulations for sample data
set.seed(100)
## Input
S &lt;- c(100, 100, 100)
mu &lt;- c(0.10, 0.12, 0.08)
sigma &lt;- matrix(
  c(
    0.1538, 0.0243, 0.0526,
    0.0243, 0.0613, 0.0209,
    0.0526, 0.0209, 0.1236
  ), 
  nrow = 3, ncol = 3
)
## Plot
map_dfr(
  1:4,
  function(x) {
    simulation &lt;- sim_spot(S, mu, sigma) %&gt;% 
      `colnames&lt;-`(str_c(&quot;S&quot;, 1:length(S))) %&gt;% 
      as_tibble()
    
    bind_cols(
      tibble(Simulation = x, Day = 0:20), 
      simulation
    )
  }
) %&gt;% 
  mutate(Simulation = factor(str_c(&quot;Trial - &quot;, Simulation))) %&gt;% 
  pivot_longer(cols=S1:S3,names_to=&quot;Stock&quot;,values_to=&quot;Price&quot;) %&gt;% 
  ggplot(mapping = aes(x = Day, y = Price, color = Stock)) +
  geom_line() +
  geom_point() +
  facet_wrap(~Simulation)</code></pre>
<p><img src="/post/post_seven_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<p>Notice that the number of assets increase rapidly the size of the covariance matrix, and reduce the performance of the algorithm. A recommneded approach to reduce the dimensionality of the problem would be using <a href="https://en.wikipedia.org/wiki/Principal_component_analysis">Principal Component Analysis</a>. This method reduce the size of the covariance matrix based on the relative explanatory power of each asset, depending on the relative weight of its eigenvalue. I will leave this topic for another post.</p>
</div>
</div>
<div id="references" class="section level2">
<h2>References</h2>
<p><em>Ackleh, A. S., Allen, E. J., Hearfott, R. B., &amp; Seshaiyer, P. (2009). Classical and Modern Numerical Analysis (1st ed., Vol. 1). Boca Raton, Florida: CRC Press.</em></p>
<p><em>Hull, J. C. (2015). Risk Management and Financial Institutions (4th ed., Vol. 1). Hoboken, New Jersey: Wiley.</em></p>
<p><em>Jones, O., Maillardet, R., &amp; Robinson, A. (2009). Introduction to Scientific Programming and Simulation Using R (1st ed., Vol. 1). Boca Raton, Florida: CRC Press.</em></p>
<p><em>Jorion, P. (2011). Financial Risk Manager Handbook (6th ed., Vol. 1). Hoboken, New Jersey: John Wiley &amp; Sons, Inc.</em></p>
<p><em>Knuth, D. E. (1998). The Art of Computer Programming (3th ed., Vol. 2). United States of America: Addison Wesley Longman.</em></p>
</div>
